{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "83a0a3e1",
      "metadata": {},
      "source": [
        "### **Transfer Learning on TinyLlama with PEFT (LoRA)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ade3b739",
      "metadata": {
        "id": "ade3b739"
      },
      "source": [
        "### Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c6614e8",
      "metadata": {
        "id": "3c6614e8"
      },
      "outputs": [],
      "source": [
        "!pip install peft accelerate transformers datasets trl bitsandbytes tensorflow nltk absl-py rouge-score sacrebleu bert-score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53a1896d",
      "metadata": {
        "id": "53a1896d"
      },
      "source": [
        "### Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d0e04a9",
      "metadata": {
        "id": "1d0e04a9",
        "outputId": "178afe6d-dc4a-4485-df63-c375dde07493"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import re\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from datasets import Dataset\n",
        "from transformers.trainer_utils import set_seed\n",
        "from evaluate import load as load_metric\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1635f2f2",
      "metadata": {
        "id": "1635f2f2"
      },
      "source": [
        "### Setting seed value for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dc5db79",
      "metadata": {
        "id": "7dc5db79"
      },
      "outputs": [],
      "source": [
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f64a03c",
      "metadata": {
        "id": "2f64a03c"
      },
      "source": [
        "### Function to clean text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36734155",
      "metadata": {
        "id": "36734155"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r\"[“”\\\"'`]+\", \"\", text)\n",
        "    text = re.sub(r\"\\n\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90a907cd",
      "metadata": {
        "id": "90a907cd"
      },
      "source": [
        "### Function to load and split data into **train and test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fa8ab75",
      "metadata": {
        "id": "6fa8ab75"
      },
      "outputs": [],
      "source": [
        "def load_and_split_data(sample_size=2000):\n",
        "    df = pd.read_parquet(\"hf://datasets/rag-datasets/rag-mini-bioasq/data/test.parquet/part.0.parquet\")\n",
        "    df.dropna(subset=['question', 'answer'], inplace=True)\n",
        "    df['text'] = df.apply(lambda x: f\"Question: {x['question']}\\nAnswer: {x['answer']}\", axis=1)\n",
        "    if sample_size is not None and len(df) > sample_size:\n",
        "        df = df.sample(sample_size, random_state=42).reset_index(drop=True)\n",
        "    train_df, test_df = train_test_split(df[['text']], test_size=0.2, random_state=42)\n",
        "    return DatasetDict({\n",
        "        \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
        "        \"test\": Dataset.from_pandas(test_df.reset_index(drop=True))\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1db6323",
      "metadata": {
        "id": "a1db6323"
      },
      "outputs": [],
      "source": [
        "dataset = load_and_split_data(sample_size=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4786981",
      "metadata": {
        "id": "b4786981"
      },
      "source": [
        "### Function to **tokenize** and **preprocess data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "677d1dcc",
      "metadata": {
        "id": "677d1dcc"
      },
      "outputs": [],
      "source": [
        "def preprocess(example, tokenizer, max_length=256):\n",
        "    encoding = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length\n",
        "    )\n",
        "    encoding[\"labels\"] = encoding[\"input_ids\"].copy()  # Ensure labels are present for CausalLM\n",
        "    return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fd1d1a6",
      "metadata": {
        "id": "1fd1d1a6"
      },
      "outputs": [],
      "source": [
        "def tokenize_dataset(dataset, tokenizer):\n",
        "    return dataset.map(lambda x: preprocess(x, tokenizer), batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52ca688f",
      "metadata": {
        "id": "52ca688f"
      },
      "outputs": [],
      "source": [
        "def get_tokenizer(model_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "094ca6cc",
      "metadata": {
        "id": "094ca6cc",
        "outputId": "64f35b82-23cc-4df9-8ebd-133efbf0ae44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 480/480 [00:00<00:00, 4580.33 examples/s]\n",
            "Map: 100%|██████████| 120/120 [00:00<00:00, 3583.37 examples/s]\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = get_tokenizer(MODEL_NAME)\n",
        "tokenized_dataset = tokenize_dataset(dataset, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cbc1e94",
      "metadata": {
        "id": "6cbc1e94"
      },
      "source": [
        "### Setting up the model with **LoRA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3deee74c",
      "metadata": {
        "id": "3deee74c"
      },
      "outputs": [],
      "source": [
        "def setup_model_with_lora(model_name):\n",
        "    from transformers import AutoConfig\n",
        "\n",
        "    config = AutoConfig.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        config=config,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        device_map=None  # avoid lazy loading to meta\n",
        "    )\n",
        "    model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # ensure proper loading\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=8,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        target_modules=[\"q_proj\", \"v_proj\"]\n",
        "    )\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d9f79cc",
      "metadata": {
        "id": "7d9f79cc",
        "outputId": "7ec0ac5a-59f4-42a4-f804-2806bd18e2eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n"
          ]
        }
      ],
      "source": [
        "model = setup_model_with_lora(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0e563ff",
      "metadata": {
        "id": "a0e563ff"
      },
      "source": [
        "### Setting up the **training process**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc03c723",
      "metadata": {
        "id": "dc03c723"
      },
      "outputs": [],
      "source": [
        "def setup_training(model, tokenizer, dataset, num_of_epochs):\n",
        "    args = TrainingArguments(\n",
        "        output_dir=\"../models/lora_bioasq_tinyllama\",\n",
        "        per_device_train_batch_size=2,\n",
        "        num_train_epochs=num_of_epochs,\n",
        "        learning_rate=2e-4,\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.1,\n",
        "        fp16=False,\n",
        "        gradient_accumulation_steps=1,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=25,\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"test\"],\n",
        "        # tokenizer=tokenizer,\n",
        "    )\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d80ec6d",
      "metadata": {
        "id": "6d80ec6d",
        "outputId": "d834f93c-4d3c-42e6-a037-21a0d578094d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "c:\\Users\\User\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='480' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [480/480 3:10:23, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>4.647300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.869300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.686800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.580800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.636000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.586400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.622500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.598900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.495100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.543800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.567300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.635400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.651900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.547200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.576300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.615900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>0.604100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.525200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>0.550500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=480, training_loss=0.8134771890938282, metrics={'train_runtime': 11445.7484, 'train_samples_per_second': 0.084, 'train_steps_per_second': 0.042, 'total_flos': 1527111525335040.0, 'train_loss': 0.8134771890938282, 'epoch': 2.0})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NO_OF_EPOCHS = 2\n",
        "trainer = setup_training(model, tokenizer, tokenized_dataset, num_of_epochs=NO_OF_EPOCHS)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6f1b45c",
      "metadata": {
        "id": "e6f1b45c"
      },
      "source": [
        "### Saving the model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "216574e9",
      "metadata": {
        "id": "216574e9"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"../models/lora_bioasq_tinyllama\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c85f61",
      "metadata": {
        "id": "b1c85f61",
        "outputId": "ca09285c-6a09-4767-eaaf-ee2078f0265a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('../models/lora_bioasq_tinyllama\\\\tokenizer_config.json',\n",
              " '../models/lora_bioasq_tinyllama\\\\special_tokens_map.json',\n",
              " '../models/lora_bioasq_tinyllama\\\\chat_template.jinja',\n",
              " '../models/lora_bioasq_tinyllama\\\\tokenizer.json')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.save_pretrained(\"../models/lora_bioasq_tinyllama\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31388615",
      "metadata": {
        "id": "31388615"
      },
      "source": [
        "### **Validation Loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "251eb0f3",
      "metadata": {
        "id": "251eb0f3",
        "outputId": "60a8f26b-2628-4d26-d841-41668d46d3fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 07:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss (eval_loss): 0.5355337858200073\n"
          ]
        }
      ],
      "source": [
        "eval_metrics = trainer.evaluate()\n",
        "print(f\"Validation Loss (eval_loss): {eval_metrics.get('eval_loss', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c184d88f",
      "metadata": {
        "id": "c184d88f"
      },
      "outputs": [],
      "source": [
        "def generate_answer(model, tokenizer, prompt):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=100,\n",
        "            num_return_sequences=1,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35ca7b57",
      "metadata": {
        "id": "35ca7b57",
        "outputId": "518a8378-1d2e-4643-b87a-51c9865906e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the treatment for tuberculosis?\n",
            "Answer: Tuberculosis is a bacterial infection caused by Mycobacterium tuberculosis. Treatment for tuberculosis is based on the patient's stage of disease, the type of tuberculosis, and the patient's immune status.\n"
          ]
        }
      ],
      "source": [
        "example_prompt = \"Question: What is the treatment for tuberculosis?\\nAnswer:\"\n",
        "print(generate_answer(model, tokenizer, example_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8370437d",
      "metadata": {
        "id": "8370437d"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a9188df",
      "metadata": {
        "id": "0a9188df"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, tokenizer, dataset, label, max_samples=30):\n",
        "    rouge = load_metric(\"rouge\")\n",
        "    bertscore = load_metric(\"bertscore\")\n",
        "    meteor = load_metric(\"meteor\")\n",
        "    bleu = load_metric(\"bleu\")\n",
        "\n",
        "    refs = []\n",
        "    preds = []\n",
        "    print(f\"\\nEvaluating on {label} set (first {max_samples} samples)...\")\n",
        "    for i, example in enumerate(dataset):\n",
        "        if i >= max_samples:\n",
        "            break\n",
        "        question = example['text'].split('\\n')[0]\n",
        "        prompt = f\"{question}\\nAnswer:\"\n",
        "        pred = generate_answer(model, tokenizer, prompt)\n",
        "        preds.append(pred.strip())\n",
        "        refs.append(example['text'].split('\\n')[1].replace('Answer: ', '').strip())\n",
        "        if i % 5 == 0:\n",
        "            print(f\"Sample {i+1}/{max_samples} done.\")\n",
        "\n",
        "    rouge_result = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
        "    bert_result = bertscore.compute(predictions=preds, references=refs, lang=\"en\")\n",
        "    meteor_result = meteor.compute(predictions=preds, references=refs)\n",
        "    bleu_result = bleu.compute(\n",
        "        predictions=[' '.join(pred.split()) for pred in preds],\n",
        "        references=[[' '.join(ref.split())] for ref in refs]\n",
        "    )\n",
        "\n",
        "    print(\"\\nROUGE-L Score:\", rouge_result['rougeL'])\n",
        "    print(\"BERTScore F1:\", sum(bert_result['f1']) / len(bert_result['f1']))\n",
        "    print(\"METEOR Score:\", meteor_result['meteor'])\n",
        "    print(\"BLEU Score:\", bleu_result['bleu'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d10c2b8",
      "metadata": {
        "id": "9d10c2b8"
      },
      "source": [
        "### Evaluation on **training set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "104267da",
      "metadata": {
        "id": "104267da",
        "outputId": "9562d5d4-f8f2-4bc1-d5ba-7962510960f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating on train set (first 30 samples)...\n",
            "Sample 1/30 done.\n",
            "Sample 6/30 done.\n",
            "Sample 11/30 done.\n",
            "Sample 16/30 done.\n",
            "Sample 21/30 done.\n",
            "Sample 26/30 done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ROUGE-L Score: 0.25717784755980444\n",
            "BERTScore F1: 0.8693346540133159\n",
            "METEOR Score: 0.2840768499963113\n",
            "BLEU Score: 0.08057544283108803\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(model, tokenizer, dataset[\"train\"], label=\"train\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38ed4490",
      "metadata": {
        "id": "38ed4490"
      },
      "source": [
        "### Evaluation on **testing set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6117fc43",
      "metadata": {
        "id": "6117fc43",
        "outputId": "38885d05-4793-4800-cfc3-2db51ca21a29"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating on test set (first 30 samples)...\n",
            "Sample 1/30 done.\n",
            "Sample 6/30 done.\n",
            "Sample 11/30 done.\n",
            "Sample 16/30 done.\n",
            "Sample 21/30 done.\n",
            "Sample 26/30 done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ROUGE-L Score: 0.26234347192124596\n",
            "BERTScore F1: 0.8629373371601105\n",
            "METEOR Score: 0.2918086697696288\n",
            "BLEU Score: 0.05961356375973927\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(model, tokenizer, dataset[\"test\"], label=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4891002",
      "metadata": {
        "id": "b4891002"
      },
      "source": [
        "### Displaying **Token Importance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c03f676f",
      "metadata": {
        "id": "c03f676f"
      },
      "outputs": [],
      "source": [
        "def input_token_importance(model, tokenizer, prompt, max_new_tokens=20):\n",
        "    \"\"\"\n",
        "    Print input tokens that most influence the generated answer.\n",
        "    \"\"\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids[0]\n",
        "    orig_output = generate_answer(model, tokenizer, prompt)\n",
        "    orig_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    print(f\"Original generated answer: {orig_output}\")\n",
        "    for i in range(1, len(input_ids)-1):  # skip special tokens\n",
        "        perturbed_ids = torch.cat([input_ids[:i], input_ids[i+1:]])\n",
        "        perturbed_prompt = tokenizer.decode(perturbed_ids, skip_special_tokens=True)\n",
        "        perturbed_output = generate_answer(model, tokenizer, perturbed_prompt)\n",
        "        if orig_output != perturbed_output:\n",
        "            print(f\"Token '{orig_tokens[i]}' is IMPORTANT: changes output to: {perturbed_output}\")\n",
        "        else:\n",
        "            print(f\"Token '{orig_tokens[i]}' is not important.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "533a4d78",
      "metadata": {
        "id": "533a4d78",
        "outputId": "dcd90ba9-8324-4c43-e3d3-398ebdf322fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original generated answer: Question: What is the treatment for tuberculosis?\n",
            "Answer: Tuberculosis is a bacterial infection caused by Mycobacterium tuberculosis. Treatment for tuberculosis is based on the patient's stage of disease, the type of tuberculosis, and the patient's immune status.\n",
            "Token '▁Question' is IMPORTANT: changes output to: : What is the treatment for tuberculosis?\n",
            "Answer: Tuberculosis is a bacterial infection caused by Mycobacterium tuberculosis. It is a major cause of death worldwide.\n",
            "Token ':' is IMPORTANT: changes output to: Question What is the treatment for tuberculosis?\n",
            "Answer: The treatment for tuberculosis is multidrug therapy.\n",
            "Token '▁What' is IMPORTANT: changes output to: Question: is the treatment for tuberculosis?\n",
            "Answer: The treatment for tuberculosis is multidrug therapy with isoniazid, rifampicin, and pyrazinamide.\n",
            "Token '▁is' is IMPORTANT: changes output to: Question: What the treatment for tuberculosis?\n",
            "Answer: Tuberculosis is a bacterial infection caused by Mycobacterium tuberculosis. It is a major cause of death worldwide. The treatment for tuberculosis is multidrug therapy with isoniazid, rifampicin, and pyrazinamide.\n",
            "Token '▁the' is IMPORTANT: changes output to: Question: What is treatment for tuberculosis?\n",
            "Answer: Treatment for tuberculosis is based on the use of antibiotics.\n",
            "Token '▁treatment' is IMPORTANT: changes output to: Question: What is the for tuberculosis?\n",
            "Answer: The for tuberculosis is a protein that is expressed in the cytoplasm of macrophages and is involved in the activation of the phagocytic process.\n",
            "Token '▁for' is IMPORTANT: changes output to: Question: What is the treatment tuberculosis?\n",
            "Answer: Tuberculosis is a bacterial infection caused by Mycobacterium tuberculosis. It is a major cause of death worldwide.\n",
            "Token '▁t' is IMPORTANT: changes output to: Question: What is the treatment foruberculosis?\n",
            "Answer: Bacteriophage therapy is a new treatment for tuberculosis.\n",
            "Token 'uber' is IMPORTANT: changes output to: Question: What is the treatment for tculosis?\n",
            "Answer: Tularemia is a zoonotic disease caused by the bacterium Francisella tularensis. The treatment for tularemia is antibiotics.\n",
            "Token 'cul' is IMPORTANT: changes output to: Question: What is the treatment for tuberosis?\n",
            "Answer: Tuberosis is a common condition that affects the joints of the feet. It is caused by a bacterial infection of the joints. The treatment for tuberosis is usually surgical.\n",
            "Token 'osis' is IMPORTANT: changes output to: Question: What is the treatment for tubercul?\n",
            "Answer: Tuberculosis is a bacterial infection caused by Mycobacterium tuberculosis. Treatment is aimed at killing the bacteria and preventing the development of tuberculosis.\n",
            "Token '?' is IMPORTANT: changes output to: Question: What is the treatment for tuberculosis\n",
            "Answer: The treatment for tuberculosis is multidrug therapy with isoniazid, rifampicin, and pyrazinamide.\n",
            "Token '<0x0A>' is IMPORTANT: changes output to: Question: What is the treatment for tuberculosis?Answer: Tuberculosis is a bacterial infection caused by Mycobacterium tuberculosis. It is a major cause of death worldwide. The treatment for tuberculosis is multidrug therapy with isoniazid, rifampicin, and pyrazinamide.\n",
            "Token 'Answer' is IMPORTANT: changes output to: Question: What is the treatment for tuberculosis?\n",
            ": Treatment for tuberculosis is multidrug therapy with isoniazid, rifampicin, and pyrazinamide.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Question: What is the treatment for tuberculosis?\\nAnswer:\"\n",
        "input_token_importance(model, tokenizer, prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08V__EWCkqgR",
      "metadata": {
        "id": "08V__EWCkqgR"
      },
      "source": [
        "# Fine tuning for TinyLlama"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sRBrKM3Jt3Id",
      "metadata": {
        "id": "sRBrKM3Jt3Id"
      },
      "source": [
        "### First Round of Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zV15dJ64lUl3",
      "metadata": {
        "id": "zV15dJ64lUl3"
      },
      "source": [
        "Reload the Dataset and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yw9gJrN9ktJV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "0bbb307514584d479dd1bc8b34454871",
            "9c5789c6120c43fe83189abc630a6113",
            "ee4a9bed6ae34873aafb3a2f374cb87c",
            "fee15bcea0f94aefab689ed3f5e5d232",
            "e6ce4148a01948b890fa0e62eb0d71e2",
            "3d1797d4efc248e99ae9151906143819",
            "bba0f9b346f84e909b7c2bcc72ad5d21",
            "339ec2eda7e14422b5fcf4e5f4f00689",
            "9b05c434b0b64e40a96875fac96fd99b",
            "ec8e02a936d348a381edfdb7b9a5470e",
            "a8a9a8c703bf4083ae2c4d89efefc564",
            "8e04c7a75f6f418bb7c42ee8ac9cdea7",
            "764bb66674d34d119011b39767e9fabc",
            "6b9e6ddbf2124da4bc0a10a9457396fb",
            "27c03e23a08b481ab36301e06b18dc13",
            "c3317f8028d245f5bcb5ffce23684b23",
            "75d5fed86446434b9f83d7fca14eccbb",
            "82ad4ffd537d4a59b41b70fa888acff9",
            "12f41ad9fa1e434cb9c5dca5459a68c5",
            "3949b3dae79447ab9d1abc88458e790b",
            "ae9655fbc27c43569438ac28e7a0bc79",
            "21ba01e1759c40c08ab853554a30ebde"
          ]
        },
        "id": "yw9gJrN9ktJV",
        "outputId": "65f6d04d-2b72-4518-f137-ebea504e713b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bbb307514584d479dd1bc8b34454871",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/480 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e04c7a75f6f418bb7c42ee8ac9cdea7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Re-import necessary libraries (already in the notebook, but ensuring they're available)\n",
        "from datasets import DatasetDict, Dataset\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Reload and split the dataset (same as original)\n",
        "def load_and_split_data(sample_size=600):\n",
        "    df = pd.read_parquet(\"hf://datasets/rag-datasets/rag-mini-bioasq/data/test.parquet/part.0.parquet\")\n",
        "    df.dropna(subset=['question', 'answer'], inplace=True)\n",
        "    df['text'] = df.apply(lambda x: f\"Question: {x['question']}\\nAnswer: {x['answer']}\", axis=1)\n",
        "    if sample_size is not None and len(df) > sample_size:\n",
        "        df = df.sample(sample_size, random_state=42).reset_index(drop=True)\n",
        "    train_df, test_df = train_test_split(df[['text']], test_size=0.2, random_state=42)\n",
        "    return DatasetDict({\n",
        "        \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
        "        \"test\": Dataset.from_pandas(test_df.reset_index(drop=True))\n",
        "    })\n",
        "\n",
        "dataset = load_and_split_data(sample_size=600)\n",
        "\n",
        "# Reload tokenizer\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Tokenize dataset (reusing original preprocess function)\n",
        "def preprocess(example, tokenizer, max_length=256):\n",
        "    encoding = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length\n",
        "    )\n",
        "    encoding[\"labels\"] = encoding[\"input_ids\"].copy()\n",
        "    return encoding\n",
        "\n",
        "def tokenize_dataset(dataset, tokenizer):\n",
        "    return dataset.map(lambda x: preprocess(x, tokenizer), batched=True)\n",
        "\n",
        "tokenized_dataset = tokenize_dataset(dataset, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EMidTQd5lZ_e",
      "metadata": {
        "id": "EMidTQd5lZ_e"
      },
      "source": [
        "Update LoRA Configuration and Reload Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Wupob2Lvkwbz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wupob2Lvkwbz",
        "outputId": "910eaa04-5abe-4902-feaf-fd98a7b17715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoModelForCausalLM\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import torch\n",
        "\n",
        "def setup_model_with_lora(model_name):\n",
        "    config = AutoConfig.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        config=config,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        device_map=None\n",
        "    )\n",
        "    model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Updated LoRA configuration\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=16,  # Increased from 8 to 16\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # Added k_proj, o_proj\n",
        "    )\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "    return model\n",
        "\n",
        "# Load model with updated LoRA configuration\n",
        "model = setup_model_with_lora(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NHlcn8L-lczG",
      "metadata": {
        "id": "NHlcn8L-lczG"
      },
      "source": [
        "Update Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QuG_lqG9lBOg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuG_lqG9lBOg",
        "outputId": "029c55b6-a0df-49bc-92eb-a6046b66cddc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "def setup_training(model, tokenizer, dataset, num_of_epochs):\n",
        "    args = TrainingArguments(\n",
        "        output_dir=\"../models/lora_bioasq_tinyllama_tuned\",\n",
        "        per_device_train_batch_size=2,\n",
        "        num_train_epochs=num_of_epochs,  # Increased to 4\n",
        "        learning_rate=1e-4,  # Reduced from 2e-4\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.1,\n",
        "        fp16=False,\n",
        "        gradient_accumulation_steps=1,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=25,\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"test\"],\n",
        "    )\n",
        "    return trainer\n",
        "\n",
        "# Set up trainer with 4 epochs\n",
        "NO_OF_EPOCHS = 1\n",
        "trainer = setup_training(model, tokenizer, tokenized_dataset, num_of_epochs=NO_OF_EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GqbrVwNQlf1x",
      "metadata": {
        "id": "GqbrVwNQlf1x"
      },
      "source": [
        "Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hzS5ia6NlGMj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "hzS5ia6NlGMj",
        "outputId": "be253215-edc9-4673-b1cf-194f81a3b1a7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [240/240 2:31:53, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>3.881000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.792700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.674300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.575300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.634300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.583900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.618900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.597600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.495600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=240, training_loss=0.9584276914596558, metrics={'train_runtime': 9155.3539, 'train_samples_per_second': 0.052, 'train_steps_per_second': 0.026, 'total_flos': 766047179243520.0, 'train_loss': 0.9584276914596558, 'epoch': 1.0})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jeEOKZY9mTXO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeEOKZY9mTXO",
        "outputId": "5a81c270-684d-4c65-f790-b19ba42b9a8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./lora_bioasq_tinyllama_tuned1/tokenizer_config.json',\n",
              " './lora_bioasq_tinyllama_tuned1/special_tokens_map.json',\n",
              " './lora_bioasq_tinyllama_tuned1/chat_template.jinja',\n",
              " './lora_bioasq_tinyllama_tuned1/tokenizer.model',\n",
              " './lora_bioasq_tinyllama_tuned1/added_tokens.json',\n",
              " './lora_bioasq_tinyllama_tuned1/tokenizer.json')"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the model\n",
        "trainer.save_model(\"./lora_bioasq_tinyllama_tuned1\")\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(\"./lora_bioasq_tinyllama_tuned1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JpUZtWyylzIM",
      "metadata": {
        "id": "JpUZtWyylzIM"
      },
      "source": [
        "Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BPBJnto_lygX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611,
          "referenced_widgets": [
            "786339107e194acc81a493a5d581c514",
            "8d89c6349bed4fce8c27702b9592289c",
            "82dc9054454b49b981e9a1856edce1d1",
            "3e5875f88cd84e3e8042504e07f12ae8",
            "e80257260c114637a732e05e1da089fe",
            "8805da991788455a9e6727ebf1406ae7",
            "65eae51306c04f0d97ce167c173f066b",
            "50d30938dbae42a5a0afc155da997106",
            "5e91e20f06974125a1c1dc2241728c38",
            "691710944a2d4c06a2d19284bcad536e",
            "921f28ea5e754d83b5acbb1a2c2d3c2b",
            "d53f21ab46be41c9afaa81606c18fa09",
            "a4dd018b91ea4a35b71b4141888f0095",
            "6df429c422664a17b5608d16f5602aad",
            "af627a3433184305be26ee61e58a1d89",
            "1af31975809c455296bb0c21b0dd05f5",
            "04d505d315b34ee4974c943d435cb2b4",
            "ca469f55ff834a0b8b23d5595052b696",
            "135ff9eb686d4814b3c4610d98709199",
            "b5dabe1496574e00b039af147e033513",
            "44845e5fc9104d2aba42652dd11279f3",
            "dc9c6523f7474cfa9620ccc5fbe21ef6",
            "1f95205a3d204536bf5110452279fdd0",
            "58f4ca685b2b44b79d3ac55860468c6e",
            "98cffdc2988c40aaae22bd1311112c3c",
            "6cdf729887564c21ae0316c965984239",
            "52dfc980142c47e0beb470825f2fc3ae",
            "66debc2674b245509e4bc12202a2bba8",
            "8210180a64eb41408fd439bc799070c0",
            "37d718c748784c93b1d9c5e230cad2dd",
            "891085bc5a574740bb74ffe102081183",
            "d2f66612a364415793a3d58ba1544fb5",
            "c2329d576bde4c708c281e291d31b03d",
            "ecfe603d4fbc4d02b398481dcf86e5d0",
            "8b1598c55e96469a98a8fa18a2aa47a2",
            "f0c16ea57abc42cb94828725148177b1",
            "19685547d5b547fabb3ddd108d25bf75",
            "e831fa4aaa884327976eb5d8e163ed04",
            "e5a5483709e5400ca11fddfbf7cac8f7",
            "c0becd75221547a9a6596531ff9882ce",
            "bf4e730f0e5b4027b33a9f3aa8f528f0",
            "0d9de593ecbd4f8fa16b4c4130a30957",
            "c3c22e5f48ae473dae3b591872088ee7",
            "e1bdfad61e6841118ced53290d70d8f5",
            "ff535047602546c5850884b7d165c4e3",
            "c98e1e44515846759597711117f4d833",
            "368b1df3a85e46ac8c540f87cba7f664",
            "8de450f392874bee83a7b59410c0c515",
            "39811a19babc4761b7f52f4e69dda69f",
            "c7cee8cb84df4c4bb8340fe69acdcb24",
            "8ac67826463d437f90a94b80a42fb84b",
            "a568dbbbdbfb48cea87ce03f4cc8b776",
            "b52fba8c6c2645ab850849a021297bbd",
            "31b9b218a4f34b61a2460c4921a79d0d",
            "56428b5234584a1da2a50f1c7f60a935",
            "da6d2e60fbea4b3da05dc9f7497fd950",
            "058719e0ea614a1192e967b5cd22a642",
            "ef5b1ce00ea5482d8b2461d4453a6f8e",
            "09294cae5cde4a9e99e2ff1a82d9081c",
            "38b7881cade64abc8271a4b1cda24f8e",
            "760b7719e0bb48e7b26f68a6dad2691c",
            "e69596f75a074a149a0e018a1523ecfe",
            "5d365cc549c84c7d96755b2778651a05",
            "c4d9cf52e3e3438dbecfc18565ead9b6",
            "f9e29661f33b4b10be809f07b768db4d",
            "c621d5047d4f4ce298ed6a3b01772e01"
          ]
        },
        "id": "BPBJnto_lygX",
        "outputId": "96a723f9-5247-4f63-d029-2e7d4049e073"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating on test set (first 30 samples)...\n",
            "Sample 1/30 done.\n",
            "Sample 6/30 done.\n",
            "Sample 11/30 done.\n",
            "Sample 16/30 done.\n",
            "Sample 21/30 done.\n",
            "Sample 26/30 done.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "786339107e194acc81a493a5d581c514",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d53f21ab46be41c9afaa81606c18fa09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f95205a3d204536bf5110452279fdd0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecfe603d4fbc4d02b398481dcf86e5d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff535047602546c5850884b7d165c4e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da6d2e60fbea4b3da05dc9f7497fd950",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ROUGE-L Score: 0.24578130142550025\n",
            "BERTScore F1: 0.8618259251117706\n",
            "METEOR Score: 0.27741167751265405\n",
            "BLEU Score: 0.05358951227114722\n"
          ]
        }
      ],
      "source": [
        "from evaluate import load as load_metric\n",
        "\n",
        "def evaluate_model(model, tokenizer, dataset, label, max_samples=30):\n",
        "    rouge = load_metric(\"rouge\")\n",
        "    bertscore = load_metric(\"bertscore\")\n",
        "    meteor = load_metric(\"meteor\")\n",
        "    bleu = load_metric(\"bleu\")\n",
        "\n",
        "    refs = []\n",
        "    preds = []\n",
        "    print(f\"\\nEvaluating on {label} set (first {max_samples} samples)...\")\n",
        "    for i, example in enumerate(dataset):\n",
        "        if i >= max_samples:\n",
        "            break\n",
        "        question = example['text'].split('\\n')[0]\n",
        "        prompt = f\"{question}\\nAnswer:\"\n",
        "        pred = generate_answer(model, tokenizer, prompt)\n",
        "        preds.append(pred.strip())\n",
        "        refs.append(example['text'].split('\\n')[1].replace('Answer: ', '').strip())\n",
        "        if i % 5 == 0:\n",
        "            print(f\"Sample {i+1}/{max_samples} done.\")\n",
        "\n",
        "    rouge_result = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
        "    bert_result = bertscore.compute(predictions=preds, references=refs, lang=\"en\")\n",
        "    meteor_result = meteor.compute(predictions=preds, references=refs)\n",
        "    bleu_result = bleu.compute(\n",
        "        predictions=[' '.join(pred.split()) for pred in preds],\n",
        "        references=[[' '.join(ref.split())] for ref in refs]\n",
        "    )\n",
        "\n",
        "    print(\"\\nROUGE-L Score:\", rouge_result['rougeL'])\n",
        "    print(\"BERTScore F1:\", sum(bert_result['f1']) / len(bert_result['f1']))\n",
        "    print(\"METEOR Score:\", meteor_result['meteor'])\n",
        "    print(\"BLEU Score:\", bleu_result['bleu'])\n",
        "\n",
        "# Evaluate on test set\n",
        "evaluate_model(model, tokenizer, dataset[\"test\"], label=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rf6SeHYtl20X",
      "metadata": {
        "id": "Rf6SeHYtl20X"
      },
      "source": [
        "Test Sample Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_sSTL7WSl33B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sSTL7WSl33B",
        "outputId": "fc405166-a8c3-4628-df37-d682974766e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the treatment for tuberculosis?\n",
            "Answer: Tuberculosis is a bacterial infection caused by Mycobacterium tuberculosis. Treatment for tuberculosis is multidrug therapy with isoniazid, rifampicin, and pyrazinamide.\n"
          ]
        }
      ],
      "source": [
        "def generate_answer(model, tokenizer, prompt):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=100,\n",
        "            num_return_sequences=1,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
        "\n",
        "# Test the prompt\n",
        "example_prompt = \"Question: What is the treatment for tuberculosis?\\nAnswer:\"\n",
        "print(generate_answer(model, tokenizer, example_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nhRarN0aPiui",
      "metadata": {
        "id": "nhRarN0aPiui"
      },
      "source": [
        "###Issue Faced in First Round\n",
        "\n",
        "Issue: Decreased evaluation metrics despite improved sample output\n",
        "\n",
        "- Description: The first round improved the specificity of the tuberculosis answer, but test set metrics (ROUGE-L, METEOR, BLEU) dropped, indicating poor generalization.\n",
        "-This could be due to:\n",
        " - Insufficient Training: 1 epoch was not enough for the model to fully adapt with the increased LoRA rank (r=16), leading to underfitting or incomplete convergence.\n",
        " - Overfitting Risk: The increased model capacity (more trainable parameters with r=16 and additional target modules) may cause the model to memorize training data patterns, reducing performance on diverse test samples.\n",
        " - Dataset Limitations: The small dataset (480 training samples) and lack of input variation may limit robustness, as seen in the token importance analysis where small input changes significantly alter outputs.\n",
        " - Learning Rate: The reduced learning rate (1e-4) may still be too high for stable convergence in 1 epoch, causing suboptimal weight updates.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XH7-KWRVP5oZ",
      "metadata": {
        "id": "XH7-KWRVP5oZ"
      },
      "source": [
        "### Second Round of Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4sk4iXGP9OZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "11dfc6c77f2743b8aedbe17018962083",
            "62e0c016b2dc4b5699099802bb222855",
            "6ef9ab5039524c9cab978c24cd6e09a8",
            "053f50fd251f4eb9875c393323377537",
            "3066030627ac48c2ae15bcbfa128ca3e",
            "96e907e88df04dc38206a123413dc257",
            "8efe8e1cc59247c6911b227b08ae1a5c",
            "3f787621fd2e4628a771ce007b46d64b",
            "e0ae292f546b4f64b3123e77294411ec",
            "2327a4d86de84f19816d3e95cc89034d",
            "b0a862aea8c44970b53aed9c767bfc9f",
            "4f16454e492a45cdba3ea837bf090978",
            "aa99efc2952e45e49b19d199873f9197",
            "5bd62997eaf24bd3a66143c336c5aca6",
            "6f9dee151b2c49658463081def6c0372",
            "f0bd917897de40da90785cb784078c1d",
            "6d22f8af7d91472c899ac75f98119d91",
            "fe8e657bb003407bb4873372f11947af",
            "9ada1b6b8a994c308e264494034867f9",
            "cb573270f98348e2bd6562ccf194dc92",
            "0b92ee06b4454e3db25adbcf09baf5f9",
            "5a411dcd5a954e86a5799a9424b79d9a"
          ]
        },
        "id": "a4sk4iXGP9OZ",
        "outputId": "07f0f912-6b6e-4512-9bec-46589a1d617d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11dfc6c77f2743b8aedbe17018962083",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/960 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f16454e492a45cdba3ea837bf090978",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/240 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Re-import necessary libraries\n",
        "from datasets import DatasetDict, Dataset\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "\n",
        "# Simple paraphrase function (manual for efficiency, no external model)\n",
        "def paraphrase_question(question):\n",
        "    question = question.replace(\"Question: \", \"\")\n",
        "    synonyms = {\n",
        "        \"what is\": [\"what are\", \"describe\", \"explain\"],\n",
        "        \"treatment\": [\"therapy\", \"management\", \"cure\"],\n",
        "        \"cause\": [\"reason\", \"etiology\", \"source\"],\n",
        "        \"for\": [\"of\", \"related to\"],\n",
        "    }\n",
        "    for key, options in synonyms.items():\n",
        "        if key in question.lower():\n",
        "            question = question.replace(key, random.choice(options))\n",
        "    return f\"Question: {question}\"\n",
        "\n",
        "# Reload and augment dataset\n",
        "def load_and_split_data(sample_size=600):\n",
        "    df = pd.read_parquet(\"hf://datasets/rag-datasets/rag-mini-bioasq/data/test.parquet/part.0.parquet\")\n",
        "    df.dropna(subset=['question', 'answer'], inplace=True)\n",
        "    df['text'] = df.apply(lambda x: f\"Question: {x['question']}\\nAnswer: {x['answer']}\", axis=1)\n",
        "    if sample_size is not None and len(df) > sample_size:\n",
        "        df = df.sample(sample_size, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # Augment training data\n",
        "    augmented = []\n",
        "    for _, row in df.iterrows():\n",
        "        paraphrased_text = paraphrase_question(row['text'].split('\\n')[0]) + '\\n' + row['text'].split('\\n')[1]\n",
        "        augmented.append({'text': paraphrased_text})\n",
        "    augmented_df = pd.DataFrame(augmented)\n",
        "    df = pd.concat([df[['text']], augmented_df]).reset_index(drop=True)\n",
        "\n",
        "    train_df, test_df = train_test_split(df[['text']], test_size=0.2, random_state=42)\n",
        "    return DatasetDict({\n",
        "        \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
        "        \"test\": Dataset.from_pandas(test_df.reset_index(drop=True))\n",
        "    })\n",
        "\n",
        "dataset = load_and_split_data(sample_size=600)\n",
        "\n",
        "# Reload tokenizer\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Tokenize dataset\n",
        "def preprocess(example, tokenizer, max_length=256):\n",
        "    encoding = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length\n",
        "    )\n",
        "    encoding[\"labels\"] = encoding[\"input_ids\"].copy()\n",
        "    return encoding\n",
        "\n",
        "def tokenize_dataset(dataset, tokenizer):\n",
        "    return dataset.map(lambda x: preprocess(x, tokenizer), batched=True)\n",
        "\n",
        "tokenized_dataset = tokenize_dataset(dataset, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fnd6hc65QA7j",
      "metadata": {
        "id": "Fnd6hc65QA7j"
      },
      "source": [
        "### Update LoRA Configuration with Higher Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qW741p8hQCdr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW741p8hQCdr",
        "outputId": "a4d7a648-f032-48cc-842b-8b679c23025e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoModelForCausalLM\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import torch\n",
        "\n",
        "def setup_model_with_lora(model_name):\n",
        "    config = AutoConfig.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        config=config,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        device_map=None\n",
        "    )\n",
        "    model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Updated LoRA configuration\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.1,  # Increased from 0.05\n",
        "        bias=\"none\",\n",
        "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        "    )\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "    return model\n",
        "\n",
        "# Load model\n",
        "model = setup_model_with_lora(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hnSWUnMzQEpa",
      "metadata": {
        "id": "hnSWUnMzQEpa"
      },
      "source": [
        "### Update Training Configuration with Gradient Accumulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N8FrCnj-QIfh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8FrCnj-QIfh",
        "outputId": "2a239416-d04f-406a-cca8-208f7e3ececd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "def setup_training(model, tokenizer, dataset, num_of_epochs):\n",
        "    args = TrainingArguments(\n",
        "        output_dir=\"../models/lora_bioasq_tinyllama_tuned2\",\n",
        "        per_device_train_batch_size=2,\n",
        "        num_train_epochs=num_of_epochs,\n",
        "        learning_rate=1e-4,\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.1,\n",
        "        fp16=False,  # Set to True if using GPU\n",
        "        gradient_accumulation_steps=2,  # Simulate batch size of 4\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=25,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,  # Save checkpoint every 100 steps\n",
        "        eval_strategy=\"steps\",  # Evaluate every 100 steps\n",
        "        eval_steps=100,\n",
        "        save_total_limit=2,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"test\"],\n",
        "    )\n",
        "    return trainer\n",
        "\n",
        "# Set up trainer with 1 epoch\n",
        "NO_OF_EPOCHS = 1\n",
        "trainer = setup_training(model, tokenizer, tokenized_dataset, num_of_epochs=NO_OF_EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P7xJrf8cRI1r",
      "metadata": {
        "id": "P7xJrf8cRI1r"
      },
      "source": [
        "### Train the Model with Early Stopping Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NKh6lzS6RKvh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "NKh6lzS6RKvh",
        "outputId": "904e2090-0f71-4468-82ab-6bbbeae7d300"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='101' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [101/240 2:05:37 < 2:56:22, 0.01 it/s, Epoch 0.42/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.559100</td>\n",
              "      <td>0.594777</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Load the best checkpoint based on validation loss (manually check logs)\n",
        "# Example: If lowest eval_loss is at step 400, load that checkpoint\n",
        "from peft import PeftModel\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PeftModel.from_pretrained(model, \"./lora_bioasq_tinyllama_tuned2/checkpoint-400\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uuh_FrOnxIjn",
      "metadata": {
        "id": "uuh_FrOnxIjn"
      },
      "source": [
        "### Issue Faced in Second Round\n",
        "\n",
        "Issue: Second round of tuning stopped prematurely and took excessive time, likely due to increased dataset size (~960 samples) and computational demands of LoRA configuration (r=16, additional target modules).\n",
        "\n",
        "- Next Steps:\n",
        "  - Reduce sample_size to 400 (~640 training samples).\n",
        "  - Enable gradient_checkpointing=True to lower memory usage.\n",
        "  - Lower LoRA rank to r=12.\n",
        "  - Set tokenizer.pad_token = \"[PAD]\", update model.config.pad_token_id.\n",
        "  - Train 1 epoch, learning rate 5e-5, lora_dropout=0.1.\n",
        "  - Monitor CPU/memory; use GPU with fp16=True if available.\n",
        "  - Target ROUGE-L > 0.27, METEOR > 0.29, BLEU > 0.06.\n",
        "  - Verify output specificity and robustness with input_token_importance\n",
        "  - Document metrics, losses, outputs, and runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eCdau5VRYT6",
      "metadata": {
        "id": "5eCdau5VRYT6"
      },
      "source": [
        "### Save the Tuned Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mf7FwfQSxLn-",
      "metadata": {
        "id": "mf7FwfQSxLn-"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "trainer.save_model(\"./lora_bioasq_tinyllama_tuned2\")\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(\"./lora_bioasq_tinyllama_tuned2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uOBZgwKDRb0R",
      "metadata": {
        "id": "uOBZgwKDRb0R"
      },
      "source": [
        "### Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tiA_ohaNRdTy",
      "metadata": {
        "id": "tiA_ohaNRdTy"
      },
      "outputs": [],
      "source": [
        "from evaluate import load as load_metric\n",
        "\n",
        "def evaluate_model(model, tokenizer, dataset, label, max_samples=30):\n",
        "    rouge = load_metric(\"rouge\")\n",
        "    bertscore = load_metric(\"bertscore\")\n",
        "    meteor = load_metric(\"meteor\")\n",
        "    bleu = load_metric(\"bleu\")\n",
        "\n",
        "    refs = []\n",
        "    preds = []\n",
        "    print(f\"\\nEvaluating on {label} set (first {max_samples} samples)...\")\n",
        "    for i, example in enumerate(dataset):\n",
        "        if i >= max_samples:\n",
        "            break\n",
        "        question = example['text'].split('\\n')[0]\n",
        "        prompt = f\"{question}\\nAnswer:\"\n",
        "        pred = generate_answer(model, tokenizer, prompt)\n",
        "        preds.append(pred.strip())\n",
        "        refs.append(example['text'].split('\\n')[1].replace('Answer: ', '').strip())\n",
        "        if i % 5 == 0:\n",
        "            print(f\"Sample {i+1}/{max_samples} done.\")\n",
        "\n",
        "    rouge_result = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
        "    bert_result = bertscore.compute(predictions=preds, references=refs, lang=\"en\")\n",
        "    meteor_result = meteor.compute(predictions=preds, references=refs)\n",
        "    bleu_result = bleu.compute(\n",
        "        predictions=[' '.join(pred.split()) for pred in preds],\n",
        "        references=[[' '.join(ref.split())] for ref in refs]\n",
        "    )\n",
        "\n",
        "    print(\"\\nROUGE-L Score:\", rouge_result['rougeL'])\n",
        "    print(\"BERTScore F1:\", sum(bert_result['f1']) / len(bert_result['f1']))\n",
        "    print(\"METEOR Score:\", meteor_result['meteor'])\n",
        "    print(\"BLEU Score:\", bleu_result['bleu'])\n",
        "\n",
        "# Evaluate on test set\n",
        "evaluate_model(model, tokenizer, dataset[\"test\"], label=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6G1nxBadRfzs",
      "metadata": {
        "id": "6G1nxBadRfzs"
      },
      "source": [
        "### Test Sample Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VuZSoi8vRhYx",
      "metadata": {
        "id": "VuZSoi8vRhYx"
      },
      "outputs": [],
      "source": [
        "def generate_answer(model, tokenizer, prompt):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=100,\n",
        "            num_return_sequences=1,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
        "\n",
        "# Test the prompt\n",
        "example_prompt = \"Question: What is the treatment for tuberculosis?\\nAnswer:\"\n",
        "print(generate_answer(model, tokenizer, example_prompt))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
